# -*- coding: utf-8 -*-
"""fonctions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wCVI5UKhzfXpq_6iQoI_9Q0gnczd-MtT
"""

#bibliotheques 
import pandas as pd
# clean the data from nonsense words, to help speed up the training process

from bs4 import BeautifulSoup
import re
from nltk.tokenize import WordPunctTokenizer
tok = WordPunctTokenizer()
# because the datasets it's large we consider automate the task of annotation using KMeans

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import numpy as np
#import preprocessor as p
import requests
import json
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import classification_report
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

#pip install tweet-preprocessor

def readNames(filePath):
    names=[]
    f=open(filePath,'r')
    for i in f:
        i=i.strip()
        names.append(i.lower())
    return names

#print(readNames("/content/الجزائر.csv"))

def read_df(path):
  df = pd.read_csv(path)
  return df

#df=read_df("/content/الجزائر.csv")
#print(df)

def clean_str(text):
  pat1 = r'@[A-Za-z0-9]+'
  pat2 = r'https?://[A-Za-z0-9./]+'
  combined_pat = r'|'.join((pat1, pat2))
  search = ["أ","إ","آ","ة","_","-","/",".","،"," و "," يا ",'"',"ـ","'","ى","\\",'\n', '\t','&quot;','?','؟','!']
  replace = ["ا","ا","ا","ه"," "," ","","",""," و"," يا","","","","ي","",' ', ' ',' ',' ? ',' ؟ ',' ! ']
  text = text.replace("،", "")
  text = text.replace(".", '')
  text = text.replace("أ", 'ا')
  #remove tashkeel
  p_tashkeel = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
  text = re.sub(p_tashkeel,"", text)
    
  #remove longation
  p_longation = re.compile(r'(.)\1+')
  subst = r"\1\1"
  text = re.sub(p_longation, subst, text)
    
  text = text.replace('وو', 'و')
  text = text.replace('يي', 'ي')
  text = text.replace('اا', 'ا')
    
  soup = BeautifulSoup(text, 'lxml')
  souped = soup.get_text()
  stripped = re.sub(combined_pat, '', souped)
  try:
    clean = stripped.decode("utf-8-sig").replace(u"\ufffd", "?")
  except:
    clean = stripped
    
  words = tok.tokenize(clean)
  return (" ".join(words)).strip()

# applying the cleaning function
def cleaning(df):
  print("Cleaning and parsing the tweets...\n")
  size = len(df)
  clean_tweet_texts = []
  for i in range(0, size):
    clean_tweet_texts.append(clean_str(df['comments'][i]))
  df['clean_text'] = clean_tweet_texts
  return df


#print(cleaning(df))

def get_user(text):
  import re
  try:
    found = re.search('@(.+?) ', text).group(1)
  except AttributeError:
    found = '' # apply your error handling
  return found

#print(get_user("@chahrazed: told us that no "))

def get_user_df(df):
  liste=[]
  for i in df["comments"]:
    liste.append(get_user(i))
  df["user"]=liste
  return df["user"]
#print(get_user_df(df))
#print(df["user"][3])

'''
This function offers the ability to predict the sentiment of a single sentence 
through the API, the sentiment is one of three classes (positive negative, neutral)
Input: 
        sentence(str): the input sentence of which the sentiment is to be predicted
Output:
        prediction(str): the sentiment of the given sentence 
'''
def predict(sentence):
    url = "http://mazajak.inf.ed.ac.uk:8000/api/predict"
    to_sent = {'data': sentence}
    data = json.dumps(to_sent)
    headers = {'content-type': 'application/json'}
    # sending get request and saving the response as response object
    response = requests.post(url=url, data=data, headers=headers)

    prediction = json.loads(response.content)['data']

    return prediction

'''
This function offers the ability to predict the sentiment of a list of sentences
through the API, the sentiment is one of three classes (positive negative, neutral)
Input: 
        sent_lst(list of str): the input list of which the sentiment of its sentences is to be predicted
Output:
        prediction(list of str): the sentiments of the given sentences
'''


def predict_list(sent_lst):
    url = "http://mazajak.inf.ed.ac.uk:8000/api/predict_list"
    to_sent = {'data': sent_lst}
    data = json.dumps(to_sent)
    headers = {'content-type': 'application/json'}
    # sending get request and saving the response as response object
    response = requests.post(url=url, data=data, headers=headers)

    prediction = json.loads(response.content)['data']

    return prediction
'''
for x in df['clean_text']:
  print(x)

for x in df['clean_text']:
  print(predict(x))
'''
def predict_df(df):
  liste=[]
  for x in df['comments']:
    liste.append(predict(x))
  df["sentiment"]=liste  
  return df

#print(predict_df(df))

def ext_pos(df):
  return (df.loc[df['sentiment']=="positive"])
#print(ext_pos(df))

def ext_neg(df):
  return (df.loc[df['sentiment']=="negative"])
#print(ext_neg(df))

def ext_net(df):
  return (df.loc[df['sentiment']=="neutral"])
#print(ext_net(df))
'''
gr = df.groupby("sentiment")
gr

nb = gr.count()
nb.sort_index(ascending=False).head()
'''
def nb_pos(df):
  x=df[df["sentiment"] == "positive"].count()
  nb=x["sentiment"]
  return(nb)
#print(nb_pos(df))

def nb_neg(df):
  x=df[df["sentiment"] == "negative"].count()
  nb=x["sentiment"]
  return(nb)
#print(nb_neg(df))

def nb_net(df):
  x=df[df["sentiment"] == "neutral"].count()
  nb=x["sentiment"]
  return(nb)
#print(nb_net(df))
'''
#model SVM
# vectorize the data 
cv = CountVectorizer()
X_data = cv.fit_transform(df['comments'].values.astype('U'))

tfidf = TfidfTransformer()
X_data_tfidf = tfidf.fit_transform(X_data)

# segment the data to train and test sets

X_train, X_test, y_train, y_test = train_test_split(X_data_tfidf,df['sentiment'], test_size=0.33, random_state=42)

X_train.shape, X_test.shape

#svm
svc = SVC()
svc.fit(X_train, y_train)

preds = svc.predict(X_test)
acc = np.mean(preds == y_test)
#print('SVC model accuracy: {}'.format(acc*100))

cr = classification_report(y_test, svc.predict(X_test))
#print(cr)

#knn
# instantiate the model with k=9
knn_9 = KNeighborsClassifier(n_neighbors=3)


# fit the model to the training set
knn_9.fit(X_train, y_train)


# predict on the test-set
y_pred_9 = knn_9.predict(X_test)


#print('Model accuracy score with k=3 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_9)))

from sklearn.metrics import classification_report

#print(classification_report(y_test, y_pred_9))

#df.to_csv(r'data.csv', index = False, header=True)




'''




